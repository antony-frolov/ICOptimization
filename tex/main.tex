% Класс документа
\documentclass[12pt]{article}
% Кодировка шрифтов
\usepackage[T2A]{fontenc}
% Кодировка файла
\usepackage[utf8x]{inputenc}
% Правила переносов для русского языка
\usepackage[english, russian]{babel}
% Нумерация страниц
\pagestyle{plain}
% Пакет для отображения графиков
\usepackage{graphicx}
\graphicspath{{../plots/}}
% Позиционирование таблиц и картинок
\usepackage{subcaption}
% Математические формулы
\usepackage{amsmath}
% Математические символы
\usepackage{amssymb}
\usepackage{mathtools}
% Задание полей
\usepackage{geometry}
% \textheight=26cm % высота текста
% \textwidth=18cm % ширина текста
% \oddsidemargin=-1cm % отступ от левого края
% \topmargin=-2.5cm % отступ от верхнего края
% Гиперссылки
\usepackage{hyperref}
% Плавающие окружения
\usepackage{float}
% Отступы в первом абзаце
\usepackage{indentfirst}
% Красивые списки
\usepackage{enumitem}
% Вставки кода
\usepackage{verbatim}
% Улучшение выравнивания
\usepackage{microtype}
% Форматирование оглавления
\usepackage{tocloft}
% Графические рисунки
\usepackage{tikz}
% BibTex
\usepackage{cite}

% Настройки

% Отступы перед абзацами
\setlength{\parindent}{1cm}
% Межстрочный интервал
\renewcommand{\baselinestretch}{1.2}

% Поля
\geometry{
    a4paper,
    left=2cm,
    right=2cm,
    top=2cm,
    bottom=2cm
}

% Цвета
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    % filecolor=magenta,
    citecolor=magenta,
    urlcolor=cyan,
}

\hypersetup{
    pdfauthor={Antony Frolov @antony-frolov},
    pdftitle={ICOptimization}
}

% Текст документа
\begin{document}

\begin{titlepage}
\begin{center}

    \includegraphics[width=9cm]{msu.eps}

    \bigskip

    Московский государственный университет имени М. В. Ломоносова

    Факультет вычислительной математики и кибернетики

    Кафедра математических методов прогнозирования

    \vspace{1.5cm}

    % {\large Фролов Антон Алексеевич}

    \vspace{1.5cm}

    \textbf{\LARGE Комбинаторная оптимизация с помощью нейронных сетей}

    \vspace{0.5cm}

    {\Large{Combinatorial Optimization using Neural Networks}}

    \vspace{1.5cm}

    {\large КУРСОВАЯ РАБОТА}

    \vspace{3cm}

    \begin{flushright}
        \parbox{0.4\textwidth}{
            \textbf{Выполнил:}\\
            студент 3 курса 317 группы\\
            \textit{Фролов Антон Алексеевич}
        }
    \end{flushright}
    
    % \vspace{0.5cm}
    
    \begin{flushright}
        \parbox{0.4\textwidth}{
            \textbf{Научный руководитель:}\\
            \textit{Кропотов Дмитрий Александрович}
        }
    \end{flushright}

    \vspace{\fill}
    Москва, 2022
\end{center}
\end{titlepage}

\newpage
\tableofcontents
\newpage

\section{Введение}

Задачи комбинаторной оптимизации зачастую являются NP-трудными и для них не существует эффективных алгоритмов решения. В общем случае для нахождения оптимального решения необходимо перебрать все возможные значения переменных, количество комбинаций которых экспоненциально зависит от размерности задачи. Поэтому для таких задач применяются различные эвристики, помогающие в среднем уменьшить время поиска оптимального решения, однако не гарантирующие быструю сходимость на любых входных данных. Примером такой эвристики может послужить метод ветвей и границ, который мы рассмотрим далее.

Эффективность нейронных сетей во многих областях машинного обучения делает их привлекательными для решения задач комбинаторной оптимизации и в частности задач целочисленного программирования. Нейронные сети могут решать поставленную задачу как целиком, получая на вход условия задачи, и выдавая ответ на выходе \cite{zhou2020learning, NIPS2015_29921001}, так и встраиваться в существующие алгоритмы оптимизации, ускоряя их \cite{BENGIO2021405, Nair2020SolvingMI}. В частности, была предложена эффективная архитектура, решающая задачу коммивояжера \cite{NIPS2015_29921001}.

В этой работе будет рассмотрена одна из задач целочисленного программирования, подмножества задач комбинаторной оптимизации.

\section{Постановка задачи}

Рассмотрим следующую задачу целочисленного программирования:
$$
\begin{gathered}
    {\operatornamewithlimits{minimize}_x} \ f(Hx - y) \\
    \text{subject to} \ x_n \in \mathcal{A} \subset \mathbb{Z}
\end{gathered}
$$
где $H \in \mathbb{R}^{Q \times N}$ — матрица некоторого преобразования, $y \in \mathbb{R}^Q$ — вектор наблюдаемых значений, $f(\cdot)$ — функция потерь, $x \in \mathbb{R}^N$ — искомый вектор, $\mathcal{A}$ — множество целочисленных значений, которые могут принимать элементы $x_n$ вектора $x$.

Подобная математическая задача комбинаторной оптимизации возникает в системах сотовой связи при построении матрицы прекодинга. В обработке сигналов эта задачу называют MIMO (Multiple Input Multiple Output) Detection. На практике эту задачу необходимо уметь решать достаточно быстро, так как матрица прекодинга пересчитывается каждый временной слот в 0.5 ms для нового набора пользователей и для каждого ресурсного блока по частотам. С ростом размерности сигнала эффективность классических методов комбинаторной оптимизации падает, и появляется необходимость быстро и точно аппроксимировать решение. 


\section{Нейросетевой подход}
Авторы статьи Learning for Integer-Constrained Optimization through Neural Networks with Limited Training \cite{zhou2020learning} предлагают следующий нейросетевой подход для решения поставленной задачи.

\subsection{Этапы построения модели}

Построение модели состоит из следующих этапов:

\begin{enumerate}
    \item Определяется вероятностная модель, в которой отклонение $r = Hx - y$ генерируется из распределения $P(r)$, определяемому по функции потерь $f(\cdot)$.

        Например, для квадратичной функции потерь $f(r) = r^2 = (Hx - y)^2$ ошибка $r$ будет иметь нормлаьное распределение $P(r) = \frac{1}{\sqrt{2 \pi}\sigma} \exp \left( -\frac{r^2}{2 \sigma^2} \right)$.

    \item Генерируется обучающая выборка из $K$ пар $(x, r)$ из распределений $\mathcal{U}(\mathcal{A}^N)$ и $P(r)$ соответственно:
        $$
        \bigg\{ \Big( x^{(1)}, r^{(1)} \Big), \Big( x^{(2)}, r^{(2)} \Big), \ldots, \Big( x^{(K)}, r^{(K)} \Big) \bigg\}
        $$
    
    \item Для фиксированной матрицы $H$ по паре векторов $(x, r)$ однозначно определяется вектор $y = Hx - r$. Таким образом, обучающую выборку можно представить в виде набора пар $(x, y)$:
        $$
        \bigg\{ \Big( x^{(1)}, y^{(1)} \Big), \Big( x^{(2)}, y^{(2)} \Big), \ldots, \Big( x^{(K)}, y^{(K)} \Big) \bigg\}
        $$
    
    \item Затем мы обучаем модель $\mathcal{N}$, причем $y^{(k)}$ подается на вход, а $x^{(k)}$ является желаемым выходом.
    
    \item Обучив модель, мы можем подать ей на вход вектор $y$ исходной задачи, получив на выходе предсказанное решение $\hat x$.
\end{enumerate}

\subsection{Струртура модели}
Модель $\mathcal{N}$ состоит из $N$ различных нейронных сетей одинаковой архитектуры, каждая из которых независимо приближает $n$-ую компоненту вектора $x$.

Основываясь на введенной вероятностной модели мы можем записать плотность условного распределения $p(x | y)$ в следующем виде:
    $$
    p(x_1, x_2, , \ldots, x_N | y) = p(x_1 | y) \cdot p(x_2 | x_1 y) \cdot \ldots \cdot p(x_N | x_1,  \ldots, x_{N-1},  y)
    $$

Предположим, что элементы вектора $x$ не зависят друг от друга:
    $$
    p(x_1, x_2, , \ldots, x_N | y) \approx p(x_1 | y) \cdot p(x_2 | y) \cdot \ldots \cdot p(x_N | y)
    $$

В таком случае принцип максимума правдоподобия принимает следующий вид:
    $$
    \begin{gathered}
    \max_{x \in \mathcal{A}^N} p(x_1, x_2, , \ldots, x_N | y) \approx \\
    \approx \max_{x \in \mathcal{A}^N} \big( p(x_1 | y) \cdot \ldots \cdot p(x_N | y) \big) = \\ 
    = \prod_{n=1}^N \max_{x_n \in \mathcal{A}} p(x_n | y)
    \end{gathered}
    $$

Для удобства изложения возьмем следующее множество $\mathcal{A}$:
    $$
    \mathcal{A} = \big\{ -2M - 1, -2M + 1, \dots, -1, 1, \dots, 2M - 1, 2M + 1 \big\}
    $$

Для нахождения $\arg \max\limits_{x_n \in \mathcal{A}} p(x_n | y), \ n = \overline{1, N}$ будем использовать $N$ нейросетей $\mathcal{N}_n$, называемых ADNN (Atomic Decision Neural Network), каждая из которых будет предсказывать отношения вероятностей гипотез $\{ H_0^{(2m)} : x_n = -2m + 1 \}$ и $\{ H_1^{(2m)} : x_n = -2m - 1 \}$, где $m = -M, \ldots, M$:
    $$
    L_{+-}^{(2m)}(y) = \frac{p(x_n = -2m + 1 | y)}{p(x_n = -2m - 1 | y)}
    $$

На вход сети $\mathcal{N}_n$ будет подаваться $2M+1$ сдвинутых на $2m, \ m = -M, \ldots, M$ векторов $y$ (Рис \ref{ADNN}). Такой сдвиг обусловлен тем, что:
    $$
    L_{+-}^{(2m)}(y) = \frac{p(x_n = -2m + 1 | y)}{p(x_n = -2m - 1 | y)} = \frac{p(x_n = 1 | y + 2m h_n)}{p(x_n = -1 | y + 2m h_n)}=  L_{+-}^{(0)}(y + 2m h_n)
    $$

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{ADNN.png}
    \caption{Структура нейронной сети}
    \label{ADNN}
\end{figure}

После того как получены все $L_{+-}^{(2m)}(y), \ m = \overline{-M, M}$, вероятность $p(x_n = - 2m + 1 | y)$ можно получить по следующей формуле:
    $$
    p( x_n = -2m + 1 | y) = p(x_n = -2M - 1 | y) \prod_{m' = M}^{m} L_{+-}^{(-2m')}(y)
    $$
(Предполагается, что $p(x_n = -2M - 1 | y) = const$)

Каждая из ADNN $\mathcal{N}_n$ представляет собой многослойный перцептрон, выходом которого являются два числа $\log(p(x_n = 1 | y))$ и $\log(p(x_n = -1 | y))$. В исследовании использовался перцептрон с одним скрытым слоем.

\subsection{Обучение ADNN}

Учитывая структуру нашей модели можно составить следующий набор данных для каждой из ADNN $\mathcal{N}_i$:
    $$
    \bigg\{ \Big( {+}1, \tilde y_+^{(1)} \Big), \Big( {-}1, \tilde y_-^{(1)} \Big), \ldots, \Big( {+}1, \tilde y_+^{(K)} \Big), \Big( {-}1, \tilde y_-^{(K)} \Big) \bigg\}
    $$
где
    $$
    \begin{aligned}
        \tilde y_+^{(k)} &= y^{(k)} - x_n^{(k)}h_n + h_n \\
        \tilde y_-^{(k)} &= y^{(k)} - x_n^{(k)}h_n - h_n
    \end{aligned}
    $$

В качестве функции потерь для обучения ADNN выбирается бинарная кросс-энтропия:
$$
\operatorname{LogLoss}(x) = \mathbb{I} \big [ x = 1 \big] \log \big( p(x = 1 | y) \big) + \mathbb{I} \big [ x = -1 \big] \log \big( p(x = -1 | y) \big)
$$

\subsection{Эксперименты}

В качестве метрики качества в последующих экспериментах будем использовать нормализованный на величину вектора $y$ MSE:
$$
L(x) = \frac{\|Ax - y\|^2}{\|y\|^2}
$$

\subsubsection{Фиксированная матрица \texorpdfstring{$H$}{H}}

Зафиксируем матрицу $H$ и проведем исследование поведения модели при различных параметрах задачи.

Посмотрим, как ведет себя модель в зависимости от размерности задачи. (Рис. \ref{adnn_dim}) 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{adnn_dim_epoch_train_loss.pdf}
    \quad
    \includegraphics[width=0.4\textwidth]{adnn_dim_epoch_train_mse.pdf}
    
    \includegraphics[width=0.4\textwidth]{adnn_dim_epoch_test_mse.pdf}
    \quad
    \includegraphics[width=0.4\textwidth]{adnn_time_dim.pdf}
    \caption{Графики зависимостей от размерности задачи}
    \label{adnn_dim}
\end{figure}

Можно заметить, что для всех размерностей MSE сходится к 0 раньше, чем сходится кросс-энтропия. С ростом размерности задачи MSE сходится медленнее. Время обучения линейно зависит от размерности задачи, так как для задачи размерности $N$ необходимо обучить $N$ нейросетей ADNN.

Посмотрим, как ведет себя модель в зависимости от размера обучающей выборки. (Рис. \ref{adnn_train_size}) 

Можно заметить, что MSE для всех выборок сходится к 0, однако для выборок размером меньше 128 ошибка на тестовой выборке все еще значительная, данных для решения задачи не достаточно. Можно сказать, что преимуществом модели является небольшой размер выборки, необходимой для обучения.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{adnn_train_size_epoch_train_loss.pdf}
    \quad
    \includegraphics[width=0.4\textwidth]{adnn_train_size_epoch_train_mse.pdf}
    
    \includegraphics[width=0.4\textwidth]{adnn_train_size_epoch_test_mse.pdf}
    \caption{Графики зависимостей от размера обучающей выборки}
    \label{adnn_train_size}
\end{figure}

Далее посмотрим, как ведет себя модель в зависимости от размера множества $\mathcal{A}$. (Рис. \ref{adnn_M})

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{adnn_M_epoch_train_loss.pdf}
    \quad
    \includegraphics[width=0.4\textwidth]{adnn_M_epoch_train_mse.pdf}
    
    \includegraphics[width=0.4\textwidth]{adnn_M_epoch_test_mse.pdf}
    \caption{Графики зависимостей от размера множества $\mathcal{A}$}
    \label{adnn_M}
\end{figure}

Из графиков видно, что модель хорошо справляется с небольшими задачами, но при увеличении размера множества начинает работать хуже.

\subsubsection{Случайная матрица \texorpdfstring{$H$}{H}}

На практике матрица преобразования $H$ может меняться, и хотелось бы уметь решать задачу для различных матриц с помощью одной модели. Однако описанная модель в силу своей структуры не предназначена для работы с различными матрицами и, как показали эксперименты, даже на небольших размерностях не справляется с такой задачей (Рис. \ref{adnn_random}). Несмотря на то, что кросс-энтропия убывает, главный критерий качества решения (MSE) не снижается.

\begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{adnn_random_epoch_train_loss.pdf}
    \quad
    \includegraphics[width=0.4\textwidth]{adnn_random_epoch_train_mse.pdf}
    
    \includegraphics[width=0.4\textwidth]{adnn_random_epoch_test_mse.pdf}
    \caption{Графики функций потерь при обучении на различных матрицах $H$}
    \label{adnn_random}
\end{figure}

\section{Метод ветвей и границ}

Как было сказано выше, у описанной нейросетевой архитектуры есть недостаток. Такая архитектура не позволяет позволяет эффективно решать задачу для различных матриц $H$. Для каждой новой матрицы сети придется переобучать, что затратно по времени.

В попытке решить эту проблему был использован известный метод ветвей и границ (\textbf{branch and bound}). Этот метод подходит для решения целого спектра зада целочисленного программирования и комбинаторной оптимизации.

Метод ветвей и границ основан на предположении, что множество всех возможных решений может быть разбито на несколько меньших подмножеств. Затем эти подмножества оцениваются на оптимальность, и процедура разбиения повторяется рекурсивно до тех пор, пока не будет найдено оптимальное решение.

В процессе выполнения алгоритма строится дерево решений. Корнем в этом дереве является исходная задача:
$$
\begin{gathered}
\operatorname{minimize} \ f(Ax - y) \\
\text{subject to} \ x \in \mathcal{A} \subseteq \mathbb{Z}
\end{gathered}
$$
Эта и последующие непрерывные задачи решаются методом Trust Region Reflective.

Затем для этой задачи отбрасываются ограничения на целочисленность переменных и находится решение непрерывной задачи:
$$
\begin{gathered}
\operatorname{minimize} \ f(Ax - y) \\
\text{subject to} \ x \in \mathbb{R}^n
\end{gathered}
$$

Обозначим решение непрерывной задачи как $x^*$. Значение функционала $f(Ax^* - y)$ является его нижней границей. Далее мы будем отслеживать нижние границы в текущем наборе узлов, выбирая из них тот, что имеет наименьшую нижнюю границу (наиболее оптимален).

Затем мы выбираем переменную с индексом $i$, по которой будем ветвить текущую задачу и разбиваем текущую задачу на две. В первой задаче добавим к текущим ограничениям ограничение $x_n \le \lfloor x_n^* \rfloor$, во второй — ограничение $x_n \ge \lceil x_n^* \rceil$. Далее решим непрерывные варианты полученных задач и получим две новые нижние оценки в новых вершинах.

Следующей вершиной для ветвления выберем ту, на которой достигается минимум нижней оценки.

Если для вершины при поиске непрерывного решения оказалось, что непрерывное решение совпадает с целочисленным, мы обновляем глобальную верхнюю оценку нашей задачи. Если верхняя оценка совпадает с нижней оценкой, то найдено оптимальное целочисленное решение. Иначе мы отбрасываем все те узлы, нижняя оценка которых выше найденной верхней оценки.

Далее мы повторяем процедуру ветвления для узла с наименьшей нижней оценкой до тех пор, пока не найдем оптимальное целочисленное решение.

Итак, метод ветвей и границ можно записать в виде следующей последовательности шагов:

\begin{enumerate}
    \setcounter{enumi}{-1}
    \item Решить непрерывный аналог исходной задачи в корневой вершине и получить нижнюю границу минимизируемого функционала. Инициализировать список текущих вершин корневой вершиной.
    \item Выбрать из списка текущих вершин вершину с наименьшей нижней границей.
    \item Выбрать переменную $x_i$ для ветвления и осуществить ветвление.
    \item Получить решение непрерывных задач в двух новых вершинах, посчитать нижние границы и добавить вершины в список текущих вершин.
    \item Если непрерывное решение в одной из вершин совпадает с целочисленным:
        \begin{enumerate}
            \item Положить верхнюю границу равной минимуму из текущей и найденной верхней границы.
            \item Удалить из списка текущих вершин все вершины, нижняя граница для которых больше обновленной верхней границы.
        
        \end{enumerate}
    \item Перейти к пункту 1.
\end{enumerate}

\subsection{Стратегии ветвления}

На этапе 2 необходимо выбрать переменную, по которой будет осуществляться ветвление. Правильно выбирая переменную для ветвления можно значительно уменьшить размер дерева поиска решения.

В качестве тривиальной эвристики можно предложить округлять непрерывное решение и выбирать переменную с наибольшей дробной частью (наибольшим модулем отклонения между значением в непрерывном и округленном решении). Можно сказать, что преимуществом такой эвристики является ее скорость, нужно всего лишь округлить непрерывное решение.

Однако, такая эвристика не очень эффективна и существует ряд других, более продвинутых методов ветвления. В качестве примера такого метода мы рассмотрим метод сильного ветвления (\textbf{strong branching}) \cite{10.5555/3015812.3015920}. В этом методе для выбора переменной сначала осуществляются все возможные ветвления по переменным $x_i$, в каждом из них находятся новые нижние оценки функционала $z_i^-$ для округления вниз и $z_i^+$ для округления вверх. Качество полученного ветвления оценивается следующей величиной:
$$
SB_i = \max \big\{ z_i^+ - z_i, \varepsilon \big\} \cdot \max \big\{ z_i^- - z_i, \varepsilon \big\}
$$
где $\varepsilon = 10^{-6}$.

Такая стратегия ветвления требует решения $O(N)$ непрерывных задач при каждом ветвлении, однако потенциально может уменьшить размер дерева 
поиска решения.

\subsection{Аппроксимация целочисленного решения}
Этап 4 можно выполнять не только, когда целочисленное решение полностью совпадает с непрерывным. В каждом узле можно приметить алгоритм, который по текущему состоянию задачи и ее непрерывному решению будет предлагать некоторое целочисленное решение. По этому решению можно так же обновить верхнюю границу и отбросить вершины, у которых нижняя граница выше новой верхней границы. 

В качестве тривиальной стратегии можно выбрать округление непрерывного решения до целочисленного. Используются и более сложные стратегии, однако здесь они рассмотрены не будут.

\subsection{Пример}
Приведем пример решения небольшой задачи с помощью метода ветвей и границ.

Возьмем следующие матрицу $A$ и вектор $y$:
$$
A = \begin{bmatrix} 3 & 5 \\ -2 & 7 \end{bmatrix}
\quad y = \begin{bmatrix} 2 \\ -2 \end{bmatrix}
$$

В качестве функционала $f(\cdot)$ возьмем квадрат нормы разности $\|Ax - y\|^2$.

Дерево поиска решения изображено на рисунке \ref{example}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        [sibling distance=12em, level distance=8em,
         every node/.style={shape=rectangle, draw, align=center}]
        \node{Node 1 \quad $lb = 0$ \\ $x^* = (0.7741, -0.0645)$ }
            child{node{Node 2 \quad $lb = 7.7837$ \\ $x^* = (0, -0.0540)$}
                  edge from parent node[draw=none, left] {$x_1 \le 0$}
            }
            child{node{Node 3 \quad $lb = 0.6621$ \\ $x^* = (1, -0.0675)$}
                child{node{Node 4 \quad $lb = 65$ \\ $x^* = (1, -1)$}
                      edge from parent node[draw=none, left] {$x_2 \le -1$}
                }
                child{node{Node 5 \quad $lb = 1$ \\ $x^* = (1, 0)$}
                     edge from parent node[draw=none, right] {$x_2 \ge 0$}
                }
                edge from parent node[draw=none, right] {$x_1 \ge 1$}
            }
        ;
    \end{tikzpicture}
    \caption{Пример дерева поиска решения}
    \label{example}
\end{figure}

После решения исходной непрерывной задачи в вершине Node 1 происходит ветвление по переменной $x_1$, и появляются два новых узла Node 2 и Node 3, которые добавляются в список текущих узлов. Нижняя граница меньше у Node 3, поэтому следующее ветвление осуществляется в нем (по переменной $x_2$), и в список текущих узлов добавляются два новых узла Node 4 и Node 5. Наименьшая нижняя граница из текущих вершин у вершины Node 5, кроме того в ней непрерывное решение совпадает с целочисленным, а значит найдено оптимальное целочисленное решение.

\subsection{Эксперименты}
Очевидным минусом метода ветвей и границ является то, что размер дерева будет быстро возрастать с увеличением размерности задачи. Как уже было сказано, эту проблему можно попытаться решить с помощью эффективной стратегии ветвления.

Посмотрим, как зависит количество вершин дерева поиска решение от размерности задачи. Приведем графики как для стратегии максимума дробной части, так и для метода strong branching при фиксированном размере множества $\mathcal{A} = \{-M, \ldots, M\}, \ M = 10$. (Рис. \ref{branch_and_bound_dim})

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{branch_and_bound_n_nodes_dim.pdf}
    \quad
    \includegraphics[width=0.45\textwidth]{branch_and_bound_time_dim.pdf}
    \caption{Графики зависимости времени и количества вершин от размерности задачи}
    \label{branch_and_bound_dim}
\end{figure}

Можно заметить, что количество узлов зависит линейно от размерности задачи. В то же время зависимость времени от размерности больше, чем линейная. Это связано с тем, что в каждой вершине нужно решать непрерывную задачу, сложность решения которой также зависит от размерности задачи.

Также можно заметить, что, как и ожидалось, стратегия strong branching уменьшает число узлов в дереве. Однако перебор всех возможных ветвлений занимает много времени.

Далее исследуем зависимость количества вершин и времени от размера целочисленного множества $\mathcal{A}$ при фиксированной размерности $N = 20$. (Рис. \ref{branch_and_bound_set_size})
\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{branch_and_bound_n_nodes_set_size.pdf}
    \quad
    \includegraphics[width=0.45\textwidth]{branch_and_bound_time_set_size.pdf}
    \caption{Графики зависимости времени и количества вершин от размера множества $\mathcal{A}$}
    \label{branch_and_bound_set_size}
\end{figure}

Можно сказать, что размер множества $\mathcal{A}$ не оказывает значительного влияния ни на время работы, ни на количество вершин в дереве поиска решения. Количество вершин даже немного уменьшается с ростом $|\mathcal{A}|$, возможно задача становится проще.

\section{Заключение}
Таким образом, в этой работе были исследованы два метода решения поставленной задачи. Первый метод использует нейронные сети, и эффективно работает на задачах небольшой размерности с фиксированной матрицей $H$. Для решения задач с различными матрицами было предложено использовать метод ветвей и границ, классический метод комбинаторной оптимизации, гарантированно находящий оптимальное решение, однако проигрывающий по времени работы.

В перспективе возможно ускорение метода ветвей и границ с помощью машинного обучения. Например для ускорения метода ветвей и границ, примененного к задаче линейного программирования, авторами статьи \cite{Nair2020SolvingMI} было предложено использовать графовые сверточные сети. В дальнейшем было бы интересно попробовать перенести предложенный ими подход на рассмотренную нами задачу.

Исходный код моделей, описанных в работе, можно найти по \href{https://github.com/antony-frolov/ICOptimization}{ссылке}.

\newpage

\bibliographystyle{plain}
\bibliography{refs}

\end{document}